\section{Implementation}

\subsection{Overview}

The GenAI Advisor was developed using a Minimum Viable Product (MVP) approach, structured around four modular components: data ingestion, strategy engine, explanation generation, and a Streamlit-based user interface. The system was designed to be fully offline, prioritising data privacy, transparency, and reproducibility. Initial efforts focused on building a thin end-to-end pipeline to demonstrate feasibility and guide future sprints.

\subsection{System Architecture}

The overall system architecture comprises:
\begin{itemize}
    \item \textbf{Frontend:} Streamlit application for user interaction
    \item \textbf{Backend:} Modular Python packages handling data ingestion, strategy evaluation, and explanation generation
    \item \textbf{LLM Inference:} Mistral 8B model run locally via Ollama
    \item \textbf{Testing and Evaluation:} Pytest-driven unit tests and backtesting utilities
\end{itemize}

A modular layout allowed for parallel development and easier testing of individual system components.

\subsection{Development Process}

An MVP-first approach guided the early stages. Agile-inspired sprints were used to iteratively add functionality and refine components. Git was used for version control, and all new modules were tested before integration. (https://github.com/andreasmallios/genaiadvisor)

\subsection{Key Modules and Features}

\subsubsection{Data Ingestion}
Equity price data is retrieved from Yahoo Finance via the \texttt{yfinance} API, using a custom function designed to streamline data access and ensure reproducibility. The function \texttt{fetch\_ticker\_data} encapsulates logic for both online retrieval and local caching. When data for a given ticker symbol already exists in the local cache (CSV\ format), it is loaded directly using \texttt{pandas.read\_csv\(\)}, with the \texttt{Date} column parsed into a datetime index to facilitate time series analysis. This not only minimises reliance on external API calls, but also enhances the determinism of backtests.

In the absence of cached data, the function instantiates a \texttt{yfinance.Ticker} object and invokes \texttt{Ticker.history()} with a default window of one year and daily frequency. Metadata fields unrelated to price analysis (e.g., \texttt{Dividends}, \texttt{Stock Splits}) are explicitly excluded to avoid downstream noise. The retrieved data frame is then persisted locally to disk, ensuring future runs use identical input data unless manually refreshed.

All data is indexed by date, with explicit naming of the index to \texttt{Date} for consistency across modules. Additionally, the local directory used for caching is dynamically resolved using Python’s \texttt{os.path} utilities, enabling portability across environments. Although not shown directly in the function, timezone consistency is enforced at the point of usage through \texttt{tz\_localize}, ensuring temporal coherence across the ingestion and backtesting pipeline.

\subsubsection{Strategy Engine}

The strategy engine was originally implemented using a simple moving average (SMA) crossover, a well-known momentum indicator. However, it was later modularised and extended into a hybrid ensemble system, integrating both traditional technical indicators and machine learning predictions. Each strategy component was abstracted into its own module with a shared interface returning a \texttt{recommendation}, \texttt{reason}, and \texttt{date}, thereby enabling flexible composition and detailed introspection.

The indicators implemented include SMA crossover, Relative Strength Index (RSI), Moving Average Convergence Divergence (MACD), Bollinger Bands, and the Stochastic Oscillator. Each module performs well-defined signal computation:
\begin{itemize}
\item \textbf{SMA crossover}: Computes short- and long-term SMAs (50-day and 200-day by default) and identifies bullish or bearish crossovers as trading signals.
\item \textbf{RSI}: Quantifies momentum and detects overbought or oversold regimes, issuing BUY signals when RSI falls below 30 and SELL when above 70.
\item \textbf{MACD}: Uses exponential moving averages to detect momentum shifts via signal-line crossovers.
\item \textbf{Bollinger Bands}: Employs a 20-day SMA and standard deviation bands to detect volatility-based breakouts.
\item \textbf{Stochastic Oscillator}: Evaluates recent closing prices relative to the high-low range over a rolling window to spot potential reversals.
\end{itemize}

Additionally, a supervised machine learning layer was incorporated using both Random Forest and Logistic Regression classifiers. These were trained on historical technical features, including SMA differentials, RSI, MACD, returns, and volume, with future returns used as labels. Predictions from both models are aggregated and translated into BUY, HOLD, or SELL signals.

All individual signals are passed to a meta-layer defined in \texttt{engine.py}, where a weighted voting mechanism combines the recommendations. Each strategy is assigned a normalised weight, and the aggregate score is computed to produce the final advisory signal. A confidence score is also generated, which allows the user to assess the strength of the consensus recommendation.

This modular and explainable architecture enhances the system’s adaptability, allowing future indicators or models to be integrated with minimal changes to the existing pipeline.

\subsubsection{Explanation Generator}

To enhance transparency and user trust, the system includes a natural language explanation generator powered by a locally hosted instance of the Mistral 8B model, served via Ollama. This design deliberately avoids reliance on proprietary cloud APIs, enabling offline inference and preserving data privacy. Communication with the model is facilitated through Python’s \texttt{subprocess} module, which programmatically invokes the model via command-line interface.

Prompts are dynamically constructed using the metadata associated with each final recommendation. The resulting prompt embeds a structured format with three components: \textit{Summary}, \textit{Reason}, and \textit{Action}. These guide the model to produce consistent, human-readable rationales tailored for non-specialist audiences. The prompt instructs the model to contextualise signals such as SMA crossover, RSI, MACD, Bollinger Bands, Stochastic Oscillator, and ML Classifier outputs. It also emphasises educational tone, British English, and a strict 300-word limit to maintain clarity and concision.

By embedding these constraints in the prompt and invoking the model locally, the system achieves explainability without introducing dependencies that compromise reproducibility or user autonomy. Errors in generation are handled gracefully, returning informative fallback messages to aid debugging.

\subsubsection{Interface}

The user interface is developed using Streamlit, a Python-based rapid application framework tailored for data-centric interfaces. This decision facilitated fast prototyping and interactive visualisation, crucial during iterative development and debugging. The application enables users to input stock tickers, retrieve historical market data, view system-generated BUY, HOLD, or SELL recommendations, and inspect the rationale behind each decision.

The main interface consists of two parts: a portfolio overview and a detailed ticker analysis. The portfolio table displays summarised recommendations for all tickers within the watchlist, fetched from a CSV file. For deeper inspection, users may select a specific ticker, upon which the interface renders price history, a modular signal breakdown, and a natural language explanation. This layered interaction supports transparency and interpretability.

Behind the scenes, the app integrates tightly with the data ingestion pipeline, strategy engine, and explanation generator. Recommendations and signal details are fetched dynamically and rendered as structured text. Visual aids, such as historical closing price charts, are plotted using \texttt{matplotlib} and embedded into both the interface and downloadable PDF reports. This PDF generation, facilitated by the \texttt{fpdf} library, enables offline review and archiving.

Importantly, the interface includes clear disclaimers and educational prompts, ensuring compliance with ethical guidance and reinforcing the tool’s non-advisory role. Collectively, the Streamlit-based interface serves not only as a functional front-end, but also as a pedagogical instrument that demystifies algorithmic financial decision-making for non-experts.


\subsubsection{Testing Infrastructure}

To ensure reliability and maintainability throughout development, the system was structured with a comprehensive suite of unit tests using the \texttt{pytest} framework. Test-driven development (TDD) was adopted for core modules, with tests written prior to or alongside implementation.

The test suite covers the following modules:

\begin{itemize}
    \item \texttt{test\_data\_ingestion.py}: Verifies correct behaviour of data retrieval and caching logic. This includes testing for file creation, timestamp parsing, index labelling, and fallback behaviour when cached data is missing or malformed.

    \item \texttt{test\_strategy\_engine.py} and \texttt{test\_bollinger\_stochastic.py}: Test the technical indicator modules, including SMA, RSI, MACD, Bollinger Bands, and Stochastic Oscillator. Assertions include type validation, expected output structure, and logical consistency of BUY/SELL/HOLD signals under different market conditions.

    \item \texttt{test\_explanation\_generator.py}: Ensures that the explanation generation module produces structured outputs (Summary–Reason–Action) and handles subprocess errors gracefully. Tests also confirm compliance with length and format constraints.

    \item \texttt{test\_engine\_call.py}: Provides integration testing for the strategy engine's ensemble logic. It validates that weighted voting correctly aggregates signal outputs and produces deterministic recommendations under fixed conditions.
\end{itemize}

Tests were executed regularly during development. To resolve relative import issues caused by the nested package structure, the environment variable \texttt{PYTHONPATH=.} was set when invoking Pytest. This ensured consistent test discovery across all components without requiring restructuring of the project hierarchy.

By maintaining tight feedback loops through unit testing, the system achieved high modular integrity and reduced the risk of regressions during iterative development.

\subsubsection{Testing and Evaluation Support}

To ensure the correctness and robustness of the implementation, the system was instrumented with unit tests using the \texttt{pytest} framework. Each functional module—including data ingestion, technical indicator computation, and explanation generation—was covered by dedicated tests that validated outputs against structural and semantic expectations. For instance, the SMA, RSI, MACD, Bollinger Bands, and Stochastic Oscillator modules were tested for valid signal outputs and consistent return types. The explanation generator was similarly validated to ensure that LLM-generated text adhered to format and length constraints.

Beyond unit testing, an evaluation pipeline was implemented via a custom backtesting framework. Historical ticker data was sliced as of specified past dates, and the system was queried to generate time-local recommendations and explanations. These were then evaluated against forward price movements over configurable horizons (e.g., 30 days), enabling ex-post performance analysis. A batch wrapper automated this process across multiple tickers and time windows, with results persisted in timestamped CSVs for downstream analysis and visualisation in Jupyter notebooks.

This combined approach—functional testing and historical simulation—ensured both implementation integrity and empirical grounding. By decoupling test targets and maintaining test coverage alongside development, the system achieved high maintainability and evaluation traceability.

\subsection{Implementation Challenges}

Several technical challenges emerged during implementation, each requiring targeted debugging and design intervention.

\begin{itemize}

\item \textbf{Timezone mismatch:}\\
Historical price data retrieved via Yahoo Finance lacked explicit timezone metadata, which introduced ambiguity when slicing datasets for backtesting. This was particularly problematic when aligning cut-off dates across modules. To mitigate this, all datetime indices were explicitly localised to the \texttt{America/New\_York} timezone using the \texttt{tz\_localize} method. This normalisation ensured consistent interpretation of trading days and eliminated subtle off-by-one errors during data slicing.

\item \textbf{CSV integrity:}
Cached datasets were initially stored as CSV files to improve reproducibility and reduce API reliance. However, inconsistencies in how indices were handled during \texttt{to\_csv()} and \texttt{read\_csv()} operations led to misaligned time series and malformed DataFrames. This was addressed by explicitly setting the \texttt{index\_label} parameter during write operations and enforcing \texttt{index\_col="Date"} with \texttt{parse\_dates=True} during reads. These changes ensured referential consistency across modules and safeguarded downstream processing.

\item \textbf{Subprocess inference:}
Local model inference via the \texttt{ollama} CLI introduced challenges in handling prompt formatting, encoding, and subprocess errors. The prompt had to be programmatically composed with structured constraints while maintaining compatibility with standard input streams. Errors during model invocation (e.g., non-zero return codes) were caught and reported via Python’s \texttt{subprocess.CalledProcessError} exception handling, allowing the interface to degrade gracefully with informative fallback messages.

\item \textbf{Test discovery:}
During unit test integration, Pytest failed to resolve relative imports due to the project’s nested directory structure. This was resolved by invoking Pytest with the environment variable \texttt{PYTHONPATH=.}, thereby ensuring that all modules were discoverable relative to the project root. This solution avoided the need for path rewrites or package restructuring.

\item \textbf{Backtesting leakage:}
Initial backtest prototypes inadvertently included post-cutoff data in model inputs, leading to optimistic and invalid evaluations. This was corrected by enforcing a strict temporal cutoff: recommendations were generated using only data up to the target date, while subsequent price movements were isolated for outcome analysis. This approach ensured methodological rigour and prevented data leakage, a critical consideration in empirical evaluation.

\end{itemize}

These challenges highlight the complexity of integrating financial data pipelines, local LLM inference, and modular testing in a cohesive system. Their resolution contributed directly to the robustness and reproducibility of the final implementation.