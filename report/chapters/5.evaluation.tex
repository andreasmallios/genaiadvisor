\section{Evaluation}

Following the successful construction of the initial MVP, a systematic evaluation was carried out to assess the correctness, reliability, and effectiveness of the GenAI Advisor as advanced features were layered onto the system.

The first step in this evaluation phase was the implementation of a modular strategy engine architecture, enabling the integration of additional signals including RSI (Relative Strength Index) and MACD (Moving Average Convergence Divergence), alongside the existing SMA crossover strategy. Each of these signals was designed as a modular Python component, facilitating independent unit testing and allowing for clear traceability of each signal's contribution to the overall recommendation engine.

Test-driven development practices were applied rigorously using \texttt{Pytest}, ensuring that the advanced signals operated correctly and that the recommendation engine's logic for combining multiple signals into final recommendations remained consistent and transparent. Tests confirmed that the system produced valid outputs under a variety of market conditions, providing confidence in the robustness of the extended MVP.

To evaluate the practical performance of the recommendation signals, a batch backtesting pipeline was developed, allowing the system to be tested systematically across multiple tickers and historical dates with a defined lookahead period. This pipeline produced structured CSV outputs containing recommendations and subsequent price movements, providing a clear dataset for analysis.

An evaluation notebook was prepared using Jupyter, processing these CSV outputs to visualise the distribution of price movements following the system's recommendations and to analyse the effectiveness of each signal type in isolation and in combination. Plots such as histograms and boxplots were generated to illustrate the distribution of returns following recommendations, and initial quantitative metrics were computed, such as the proportion of BUY signals that resulted in positive returns.

These evaluation steps ensured that the extended MVP did not introduce regressions while adding advanced functionality and that it continued to align with the goals of interpretability, offline operation, and explainability. The evaluation process also provided data-driven evidence to support further refinement of signal thresholds, signal weighting, and prompt engineering for explanation generation in subsequent development sprints.

\subsubsection{Testing and Evaluation Support}

To ensure the correctness and robustness of the implementation, the system was instrumented with unit tests using the \texttt{pytest} framework. Each functional module—including data ingestion, technical indicator computation, and explanation generation—was covered by dedicated tests that validated outputs against structural and semantic expectations. For instance, the SMA, RSI, MACD, Bollinger Bands, and Stochastic Oscillator modules were tested for valid signal outputs and consistent return types. The explanation generator was similarly validated to ensure that LLM-generated text adhered to format and length constraints.

Beyond unit testing, an evaluation pipeline was implemented via a custom backtesting framework. Historical ticker data was sliced as of specified past dates, and the system was queried to generate time-local recommendations and explanations. These were then evaluated against forward price movements over configurable horizons (e.g., 30 days), enabling ex-post performance analysis. A batch wrapper automated this process across multiple tickers and time windows, with results persisted in timestamped CSVs for downstream analysis and visualisation in Jupyter notebooks.

This combined approach—functional testing and historical simulation—ensured both implementation integrity and empirical grounding. By decoupling test targets and maintaining test coverage alongside development, the system achieved high maintainability and evaluation traceability.
