\section{Feature Prototype and Evaluation}

This section presents a focused implementation of the explanation generator and a comparative evaluation of several local large language models (LLMs). The objective is to determine which model best aligns with the system’s requirements for explainability and accessibility. The evaluation is scoped to models runnable within an 8 GB VRAM constraint to support reproducibility and local deployment.

\subsection{Prototype Objective and Scope}

The prototype isolates the final layer of the GenAI Advisor architecture, the explanation generator. It takes as input a structured recommendation, typically generated by the strategy engine, and produces a textual rationale. This explanation layer is central to the system's commitment to transparency and user trust. For the purpose of this prototype, all upstream processes (data collection, feature engineering, and decision logic) are simulated using static inputs. The design focuses on testing whether modern, instruction-tuned LLMs can interpret structured financial metrics and generate plausible justifications for retail-facing outputs.

\subsection{Model Selection Rationale}

The four models selected for evaluation were chosen based on their availability through the Ollama runtime, compatibility with 8GB VRAM constraints, and representativeness of recent LLM design trends. Each model was expected to demonstrate distinctive strengths aligned with the explanation generator’s requirements: factual precision, clarity, and educative capacity.

\begin{itemize}
    \item \texttt{mistral:7b} was selected due to its strong performance on standard instruction-following benchmarks. As an open-weight model built with modern decoder architecture and dense pretraining, it has demonstrated balanced performance across summarisation, QA, and reasoning tasks. Its outputs are typically well-structured, concise, and fluid.
    
    \item \texttt{deepseek-r1:8b} is a reasoning-optimised model developed to excel at complex multi-step tasks. It remains operational on standard consumer-grade GPUs when quantised. It was chosen for its potential to deliver highly grounded and stepwise rationale construction, a valuable trait for explanatory outputs in finance.
    
    \item \texttt{llama3.1:8b} was included as Meta’s latest general-purpose model. It represents the frontier in pretraining scale and refinement, with improvements in response quality and instruction tuning. It was expected to deliver polished and high-coverage answers, potentially balancing fluency and informativeness.
    
    \item \texttt{gemma:7b} from Google was selected for its lightweight design and efficient inference profile. While it lacks the same instruction-following depth as the others, it serves as a practical baseline for evaluating trade-offs between model size, efficiency, and output utility.
\end{itemize}

These models span a useful spectrum from high-performance instruction tuning (\texttt{mistral}) and reasoning depth (\texttt{deepseek}), to baseline fluency (\texttt{llama3.1}) and efficient deployment (\texttt{gemma}). Their inclusion enables a comparative understanding of how different model classes handle structured explanation generation within the constraints and goals of this project.

\subsection{Implementation Details}

The implementation is a standalone Python script that integrates with the \texttt{Ollama} runtime to serve quantised LLMs locally. This approach avoids reliance on cloud APIs or external dependencies and ensures compatibility with standard workstation-grade hardware.

Each model received the same structured input and was prompted as follows:

\begin{verbatim}
You are a financial assistant. Based on the following metrics:

Ticker: {ticker}
Action: {action}
P/E Ratio: {pe}
RSI: {rsi}
Beta: {beta}

Generate a short, clear explanation of why this action was recommended,
using only the signals provided.

Your explanation will be evaluated based on:
1. Factual alignment – Does it correctly reference and reflect the input signals?
2. Clarity – Is it readable and jargon-free for a non-expert?
3. Educational value – Does it help the user understand financial reasoning?

Write in a tone that is simple, supportive, and instructive.
\end{verbatim}

Using this prompt, the four language models were evaluated for their ability to generate explanatory justifications from structured financial signals. Each model was tasked with producing a natural language explanation based on three key indicators: P/E ratio, RSI, and beta, along with a pre-assigned action recommendation.

Mistral 7B typically produced structured, metric-by-metric justifications. Its responses tended to follow a clear, segmented format, interpreting each signal independently before summarising the overall recommendation. The model showed a tendency to anchor its language in established financial reasoning, often framing explanations within a cautiously optimistic narrative.

DeepSeek-R1 8B adopted a more process-oriented style. It often simulated internal reasoning steps, providing commentary on its own interpretive process before delivering a final summary. This approach tended to produce verbose but richly annotated responses, balancing technical correctness with pedagogical tone.

LLaMA 3.1 8B favoured a polished and highly readable output structure. Its responses were often framed as formal recommendations, with summarised takeaways. The tone was typically confident and informative, with a focus on simplifying terminology while preserving the logic behind financial metrics.

Gemma 7B generally produced shorter, more direct explanations. Its responses prioritised brevity and tended to cover each input signal in a minimal fashion. While the output was usually factually consistent with the input, it often lacked contextual depth or elaboration.

Across all models, the task of converting quantitative financial indicators into qualitative, user-facing explanations highlighted different language generation strategies. Some models leaned toward verbosity and teaching, while others prioritised conciseness and structure. These differences reflect varying underlying instruction tuning and decoding preferences, underscoring the importance of model selection depending on the target user experience.

\subsection{Evaluation Methodology}

A peer-evaluation strategy was employed where each model was tasked with anonymously evaluating the outputs of the others. For each of the three input scenarios, four anonymised responses were scored against the following dimensions:

\begin{enumerate}
    \item \textbf{Factual alignment}: Did the response correctly use the input metrics?
    \item \textbf{Clarity}: Was the response understandable and free of jargon?
    \item \textbf{Educational value}: Did the response help the user learn financial reasoning?
\end{enumerate}

Each evaluation was repeated three times per model to control for decoding variance. All scores were normalised to a 1–5 scale. Fluency (clarity) was given a slightly higher weight (0.4) than factual and educational value (0.3 each) during ranking.

\subsection{Results and Comparison}

The final weighted scores are summarised in Table~\ref{tab:model_scores}.

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\textbf{Model} & \textbf{Factual} & \textbf{Clarity} & \textbf{Learning} \\
\hline
\texttt{deepseek-r1:8b} & 4.66 & 4.20 & 4.66 \\
\texttt{mistral:7b}     & 4.42 & 4.06 & 4.44 \\
\texttt{llama3.1:8b}    & 4.33 & 4.00 & 4.22 \\
\texttt{gemma:7b}       & 4.14 & 3.80 & 4.00 \\
\end{tabular}
\caption{Peer-evaluated model scores (1–5 scale).}
\label{tab:model_scores}
\end{table}

\subsection{Discussion of Results}

Before conducting the evaluation, it was anticipated that \texttt{mistral:7b} would lead due to its strong benchmark performance and widespread use in instruction-following tasks. Similarly, \texttt{deepseek-r1:8b} was included for its reputed strength in reasoning tasks, while \texttt{llama3.1:8b} was expected to provide fluent yet generalist output. \texttt{gemma:7b}, although efficient, was included as a performance baseline.

Interestingly, \texttt{deepseek-r1:8b} achieved the highest aggregate score, particularly due to its consistently accurate references to input signals and educational structure. It often contextualised the rationale using analogies and progressive explanation, aligning with the system's goal of financial literacy enhancement. \texttt{mistral:7b} remained competitive, delivering slightly more fluent prose but occasionally at the cost of explicit metric referencing.

\texttt{llama3.1:8b} and \texttt{gemma:7b} underperformed in clarity and interpretability, with some responses lacking actionable justification or oversimplifying the rationale. These findings reinforce the need for balance between readability and groundedness in explanation generation, a trade-off that \texttt{deepseek-r1:8b} currently handles most effectively.

\subsection{Conclusion}

Based on peer evaluations across multiple structured prompts and consistent scoring, \texttt{deepseek-r1:8b} achieved the highest aggregate marks, particularly for its detailed and pedagogically aligned explanations. However, its verbosity introduces a need for output post-processing, which adds complexity to the current system. Given the project’s focus on simplicity and low-overhead deployment, \texttt{mistral:7b} is selected as the explanation engine for the GenAI Advisor prototype. It strikes a more practical balance between fluency, interpretability, and implementation feasibility, making it better suited for the intended use case and resource constraints.
